/*
Adaption of the tweet training code to support websites with the provided .csv format. consider refactoring tweet code to trainTweetModel
*/


package isr.project

import org.apache.log4j.{Level, Logger}
import org.apache.spark.mllib.classification.LogisticRegressionModel
import org.apache.spark.mllib.feature.Word2VecModel
import org.apache.spark.mllib.linalg.Word2VecClassifier
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}


case class Website(id: String, websiteText: String, label: Option[Double] = None)


Object TrainWebsiteModel{

	//train and test website modes. currently works with readwebsitefromcsv
  def TrainWebsiteModels(trainFile: String, testFile: String, sc: SparkContext): Unit = {
    println("Training website models")

		//load local label double-string map and define parition sizes 
    val training_partitions = 8
    val testing_partitions = 8

		//load website data from .csv provided by CMW team.
    val websites_1 = getWebsitesFromFile(trainFile, 1.0, sc).collect()
    val websites_2 = getWebsitesFromFile(testFile, 2.0, sc).collect()
		
		//combine odds of both website sets for training, and evens of both website sets for testing
		def even(x): return x%2 == 0
		def odd(x): return not even(x)
		web1_odd, web1_even = (websites_1.filter(f) for f in (odd,even))
		web2_odd, web2_even = (websites_2.filter(f) for f in (odd,even))
		val rddTrainWebsites = web1_odd.union(web2_odd)
		val rddTestWebsites = web1_even.union(web2_even)

    DataStatisticsW(rddTrainWebsites, rddTestWebsites)

//TODO
    SetupWord2VecField(trainFile, getTweetsFromFile(trainFile, labelMap, sc))

    val trainTweetsRDD = sc.parallelize(trainTweets, training_partitions)

		//test that RDD map has the expected text file data
		//println("Get here")    
		//println("#############################################################")
		//for ((k,v) <- trainTweets) printf("key: %s, value: %s", k, v)
		//println(trainTweets.mkString(" "))
		//println(trainTweetsRDD)
		
		//val cleaned_trainingTweetsRDD = sc.parallelize(CleanTweet.clean(trainTweetsRDD,sc).collect(),training_partitions).cache()

    val (word2VecModel, logisticRegressionModel, _) = PerformTraining(sc, trainTweetsRDD)																
		val testTweetsRDD = sc.parallelize(testTweets, testing_partitions)
    //val cleaned_testTweetsRDD = sc.parallelize(CleanTweet.clean(testTweetsRDD,sc).collect(),testing_partitions).cache()

    PerformPrediction(sc, word2VecModel, logisticRegressionModel, testTweetsRDD)

  }

	//Gets webwsite input train/test data stats
  private def DataStatisticsW(train: Array[Website], test: Array[Website]) = {
    //place a debug point or prints to see the statistics
    val trainCount = train.length
    val testCount = test.length
    val both = train ++ test
    val numClasses = both.map(x => x.label).distinct.length
    val minClass = both.groupBy(x => x.label).map(t => (t._1, t._2.length)).valuesIterator.min
    val minClassCount = both.groupBy(x => x.label).map(t => (t._1, t._2.length)).toList.count(x => x._2 == minClass)
    val maxClass = both.groupBy(x => x.label).map(t => (t._1, t._2.length)).valuesIterator.max
    val numToAmount = both.groupBy(x => x.label).map(t => (t._1, t._2.length)).toList.groupBy(x => x._2).mapValues(_.size)
    println("Histogram begin")
    for (i <- 1 to maxClass){
      println(i + "\t" + numToAmount.getOrElse(i,0))
    }
    println("the stats have been generated")
  }


	//initial get website data from SMW .csv file. each file is its own topic already, so provide the desired label ID!
	def getWebsitesFromRawCsv(fileName:String, labelID:Double, sc: SparkContext): RDD[Website] = {
    val file = sc.textFile(fileName)
    file.map(x => x.split(",", 5)).filter(_.length == 5).map(x => Website(x(0),x(4), labelID))
  }


	//
  def SetupWord2VecField(trainFile: String, trainTweets: RDD[Tweet]): Unit = {
    Word2VecClassifier._lrModelFilename = trainFile + "lrModel"
    Word2VecClassifier._word2VecModelFilename = trainFile + "w2vModel"
    Word2VecClassifier._numberOfClasses =  trainTweets.map(x => x.label).distinct.count().toInt
  }

}


